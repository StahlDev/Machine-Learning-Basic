{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mutilayer_Perceptron.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueIJB9t5rfU3"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wpz4-OjHHTMB"
      },
      "source": [
        "def softmax_stable(Z):\n",
        "    # each row of Z is a set of score\n",
        "    e_Z = np.exp(Z - np.max(Z, axis = 1, keepdims=True))\n",
        "    A = e_Z / e_Z.sum(axis = 1, keepdims = True)\n",
        "    return A"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYGMHkHXH0xj"
      },
      "source": [
        "def crossentropy_loss(Yhat, y):\n",
        "    \"\"\"\n",
        "    Yhat: a numpy array of shape (Npoints, nClasses) -- predict output\n",
        "    y: a numpy array of shape (Npoints) -- ground truth.\n",
        "    We dont need to use one-hot display of y here since most of element are zeros. Each row of Yhat, we just need to access to the corresponding index only.\n",
        "    \"\"\"\n",
        "    return -np.mean(np.log(Yhat[range(Yhat.shape[0]), y]))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQVnclx4I4RG"
      },
      "source": [
        "def MLP_init(d0, d1, d2): # depend on number of nodes in each layer of MLP\n",
        "    \"\"\"\n",
        "    Initialize W1, b1, W2, b2\n",
        "    d0: dimention of input data\n",
        "    d1: number of hiden layer's units\n",
        "    d2: number of output units = number of classes\n",
        "    \"\"\"\n",
        "    W1 = 0.01 * np.random.randn(d0, d1)\n",
        "    b1 = np.zeros(d1)\n",
        "    W2 = 0.01 * np.random.randn(d1, d2)\n",
        "    b2 = np.zeros(d2)\n",
        "    return (W1, b1, W2, b2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jglJpVYbJsNV"
      },
      "source": [
        "def MLP_predict(X, W1, b1, W2, b2):\n",
        "    \"\"\"\n",
        "    Suppose the network has been trained, predict class of new points.\n",
        "    X: data matrix, each Row is one data point\n",
        "    W1, b1, W2, b2: learned weight matrices and biases\n",
        "    \"\"\"\n",
        "    Z1 = X.dot(W1) + b1\n",
        "    # Z1 shape of (N, d1)\n",
        "    # W1 shape of (d0, d1)\n",
        "    # b1 shape of (d1,)\n",
        "    # X shape of (N, d0)\n",
        "    # note that we dont need to transpose the W1 matrix.\n",
        "    A1 = np.maximum(Z1, 0)\n",
        "    # A1 shape of (N, d1)\n",
        "    Z2 = A1.dot(W2) + b2\n",
        "    # W2 shape of (d1, d2)\n",
        "    # b2 shape of (d2,)\n",
        "    # note that we dont need to transpose the W2 matrix\n",
        "    # Z2 shape of (N, d2)\n",
        "    Yhat = A2 = softmax_stable(Z2)\n",
        "    # A2, Yhat shape of (N, d2) because softmax_stable didnt change the shape of those.\n",
        "    return np.argmax(Z2, axis=1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUzrKC3DMgQ8"
      },
      "source": [
        "def MLP_fit(X, y, W1, b1, W2, b2, eta, batch_size = 30):\n",
        "    # eta is learning rate\n",
        "    # X shape of (N, d0)\n",
        "    # y shape of nPoints\n",
        "    lost_hist = []\n",
        "    N = X.shape[0]\n",
        "    nbatches = int(np.ceil(float(N)/batch_size))\n",
        "    for ep in range(20000): # number of epochs\n",
        "        mix_idxs = np.random.permutation(N) # stochastic\n",
        "        \n",
        "        for i in range (nbatches):\n",
        "            batch_idxs = mix_idxs[batch_size*i : min(batch_size*(i+1), N)]\n",
        "            X_batch, y_batch = X[batch_idxs], y[batch_idxs]\n",
        "            # Feed-Forward\n",
        "            Z1 = X_batch.dot(W1) + b1\n",
        "            # Z1 shape of (N, d1)\n",
        "            A1 = np.maximum(Z1, 0)\n",
        "            # A1 shape of (N, d1)\n",
        "            Z2 = A1.dot(W2) + b2\n",
        "            # Z2 shape of (N, d2)\n",
        "            Yhat = softmax_stable(Z2)\n",
        "            # Yhat shape of (N, d2)\n",
        "\n",
        "            # print lost value after each 1000 epochs\n",
        "            if ep % 1000 == 0 and i == nbatches - 1:\n",
        "                loss = crossentropy_loss(Yhat, y_batch)\n",
        "                print(\"iter %d, loss: %f\" % (ep, loss))\n",
        "                lost_hist.append(loss)\n",
        "\n",
        "            # backpropagation batch\n",
        "            id0 = range(Yhat.shape[0])\n",
        "            Yhat[id0, y_batch] -= 1\n",
        "            E2 = Yhat / X.shape[0]\n",
        "            # E2 shape of (N, d2)\n",
        "            gW2 = A1.T.dot(E2)\n",
        "            # gW2 shape of (d1, d2)\n",
        "            gb2 = np.sum(E2, axis = 0)\n",
        "            # gb2 shape of (d2,)\n",
        "            E1 = E2.dot(W2.T)\n",
        "            E1[Z1 <= 0] = 0\n",
        "            # E1 shape of (N, d1)\n",
        "            # derivative of ReLU (Z1) = 0 when e_{i} in Z1 smaller than 0, and = 1 when e_{i} in Z1 greater than 0.\n",
        "            gW1 = X_batch.T.dot(E1)\n",
        "            gb1 = np.sum(E1, axis = 0)\n",
        "            # gW1 shape of (d0, d1)\n",
        "\n",
        "            # Gradient Descent\n",
        "            W1 += -eta * gW1\n",
        "            W2 += -eta * gW2\n",
        "            b1 += -eta * gb1\n",
        "            b2 += -eta * gb2\n",
        "    return W1, b1, W2, b2, lost_hist"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uLnuRUxfDD0",
        "outputId": "8b3df45b-d01b-44a2-aef0-8478193af7e8"
      },
      "source": [
        "means = [[-1, -1], [1, -1], [0, 1]]\n",
        "cov = [[1, 0], [0, 1]]\n",
        "N = 200\n",
        "X0 = np.random.multivariate_normal(means[0], cov, N)\n",
        "X1 = np.random.multivariate_normal(means[1], cov, N)\n",
        "X2 = np.random.multivariate_normal(means[2], cov, N)\n",
        "X = np.concatenate((X0, X1, X2), axis = 0)\n",
        "y = np.asarray([0]*N + [1]*N + [2]*N)\n",
        "\n",
        "# Supose X, y are training input and output, respectively\n",
        "d0 = 2  # data dimention\n",
        "d1 = h = 2000 # number of hidden units\n",
        "d2 = C = 3 # number of class in last layer\n",
        "eta = 1 # learning rate\n",
        "(W1, b1, W2, b2) = MLP_init(d0, d1, d2)\n",
        "(W1, b1, W2, b2, lost_hist) = MLP_fit(X, y, W1, b1, W2, b2, eta)\n",
        "y_pred = MLP_predict(X, W1, b1, W2, b2 )\n",
        "acc = 100 * np.mean(y_pred == y)\n",
        "print('tranning accuracy: %.2f %%' % acc)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0, loss: 1.035338\n",
            "iter 1000, loss: 0.349936\n",
            "iter 2000, loss: 0.439657\n",
            "iter 3000, loss: 0.538261\n",
            "iter 4000, loss: 0.435431\n",
            "iter 5000, loss: 0.475841\n",
            "iter 6000, loss: 0.486963\n",
            "iter 7000, loss: 0.530784\n",
            "iter 8000, loss: 0.514993\n",
            "iter 9000, loss: 0.644424\n",
            "iter 10000, loss: 0.343045\n",
            "iter 11000, loss: 0.622766\n",
            "iter 12000, loss: 0.379253\n",
            "iter 13000, loss: 0.519054\n",
            "iter 14000, loss: 0.340559\n",
            "iter 15000, loss: 0.301515\n",
            "iter 16000, loss: 0.718444\n",
            "iter 17000, loss: 0.431808\n",
            "iter 18000, loss: 0.445064\n",
            "iter 19000, loss: 0.309719\n",
            "tranning accuracy: 80.67 %\n"
          ]
        }
      ]
    }
  ]
}